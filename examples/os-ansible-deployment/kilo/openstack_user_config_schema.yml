---
type: object
copyright: >
  Copyright 2014, Rackspace US, Inc.
 
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
 
      http://www.apache.org/licenses/LICENSE-2.0
 
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
documentation: >
  Overview
  ========
 
  This file contains the configuration for OpenStack Ansible Deployment
  (OSAD) core services. Optional service configuration resides in the
  conf.d directory.
 
  You can customize the options in this file and copy it to
  /etc/openstack_deploy/openstack_user_config.yml or create a new
  file containing only necessary options for your environment
  before deployment.
 
  OSAD implements PyYAML to parse YAML files and therefore supports structure
  and formatting options that augment traditional YAML. For example, aliases
  or references. For more information on PyYAML, see the documentation at
 
  http://pyyaml.org/wiki/PyYAMLDocumentation

common_definitions:
  infra_hosts:
    patternProperties:
      "[a-zA-Z0-9]+":
        type: object
        required:
          - ip
        properties:
          ip:
            type: string
            anyOf:
              - format: ipv4
              - format: ipv6

  volume_backend_properties:
    type: object
    required:
      - volume_backend_name
      - volume_driver
      - volume_group
    properties:
      volume_backend_name:
        type: string
        documentation: >
          Name of backend, arbitrary.
      volume_driver:
        type: string
        documentation: >
          Name of volume driver, typically one of
          'cinder.volume.drivers.lvm.LVMVolumeDriver',
          'cinder.volume.drivers.nfs.NfsDriver',
          'cinder.volume.drivers.netapp.common.NetAppDriver'.
          NB. When using NFS driver you may want to adjust your
          env.d/cinder.yml file to run cinder-volumes in containers.
          NB. When using NFS driver you may want to adjust your
          env.d/cinder.yml file to run cinder-volumes in containers.
      volume_group:
        type: string
        documentation: >
          Name of LVM volume group, typically 'cinder-volumes'.
      nfs_shares_config:
        type: string
        documentation: >
          File containing list of NFS shares available to cinder, typically
          '/etc/cinder/nfs_shares'.
      nfs_mount_point_base:
        type: string
        documentation: >
          Location in which to mount NFS shares, typically
          '$state_path/mnt'.
      # NOTE(sigmavirus24): The following are *required* if you're using the
      # NetAppDriver above
      netapp_storage_family:
        type: string
        documentation: >
          Access method, typically 'ontap_7mode' or 'ontap_cluster'.
      netapp_storage_protocol:
        type: string
        documentation: >
          Transport method, typically 'scsi' or 'nfs'. NFS transport also
          requires the 'nfs_shares_config' option.
      netapp_server_hostname:
        type: string
        documentation: >
          NetApp server hostname.
      netapp_server_port:
        type: integer
        documentation: >
          NetApp server port, typically 80 or 443.
      netapp_login:
        type: string
        documentation: >
          NetApp server username.
      netapp_password:
        type: string
        documentation: >
          NetApp server password.

  storage_host:
    required:
      - ip
      - container_vars
    type: object
    documentation: >
      Shared definition for a storage host described in 'storage_hosts'.
    properties:
      ip:
        type: string
        documentation: >
          IP address of this target host, typically the IP address assigned to
          the management bridge.
        anyOf:
          - format: ipv4
          - format: ipv6
      container_vars:
        type: object
        documentation: >
          Contains storage options for this target host.
        required:
          - cinder_backends
        properties:
          cinder_storage_availability_zone:
            type: string
            documentation: >
              Cinder availability zone.
          cinder_default_availability_zone:
            type: string
            documentation: >
              If the deployment contains more than one cinder availability zone,
              specify a default availability zone.
          cinder_backends:
            type: object
            documentation: >
              Contains cinder backends.
            properties:
              limit_container_types:
                type: string
                documentation: >
                  Container name string in which to apply these options. Typically
                  any container with 'cinder_volume' in the name.
              cinder_nfs_client:
                type: object
                documentation: >
                  Automates management of the file that cinder references for a list of
                  NFS mounts.
                required:
                  - nfs_shares_config
                  - shares
                properties:
                  nfs_shares_config:
                    type: string
                    documentation: >
                      File containing list of NFS shares available to cinder, typically
                      typically /etc/cinder/nfs_shares.
                  shares:
                    type: array
                    documentation: >
                      List of shares to populate the 'nfs_shares_config' file. Each share
                      uses the following format:
                    minItems: 1
                    items:
                      type: object
                      required:
                        - ip
                        - share
                      properties:
                        ip:
                          type: string
                          anyOf:
                            - format: ipv4
                            - format: ipv6
                        share:
                          type: string
            patternProperties:
              "(?:limit_container_types)[a-zA-Z0-9]+":
                type: object
                documentation: >
                  Arbitrary name of the backend. Each backend contains one or more
                  options for the particular backend driver. The template for the
                  cinder.conf file can generate configuration for any backend
                  providing that it includes the necessary driver options.
                properties:
                  $ref: volume_backend_properties

required:
  - cidr_networks
  - global_overrides
  - shared-infra_hosts
  - repo-infra_hosts
  - os-infra_hosts
  - identity_hosts
  - network_hosts
  - compute_hosts
  - storage-infra_hosts
  - storage_hosts
  - log_hosts

properties:
  cidr_networks:
    documentation: >
      Contains an arbitrary list of networks for the deployment. For each network,
      the inventory generator uses the IP address range to create a pool of IP
      addresses for network interfaces inside containers. A deployment requires
      at least one network for management.
     
        Option: <value> (required, string)
        Name of network and IP address range in CIDR notation. This IP address
        range coincides with the IP address range of the bridge for this network
        on the target host.
     
      Example:
     
      Define networks for a typical deployment.
     
        - Management network on 172.29.236.0/22. Control plane for infrastructure
          services, OpenStack APIs, and horizon.
        - Tunnel network on 172.29.240.0/22. Data plane for project (tenant) VXLAN
          networks.
        - Storage network on 172.29.244.0/22. Data plane for storage services such
          as cinder and swift.
     
      cidr_networks:
        management: 172.29.236.0/22
        tunnel: 172.29.240.0/22
        storage: 172.29.244.0/22
     
      Example:
     
      Define additional service network on 172.29.248.0/22 for deployment in a
      Rackspace data center.
     
        snet: 172.29.248.0/22
    type: object
    patternProperties:
      '[a-zA-Z]+':
        type: string
        format: cidr

  used_ips:
    documentation: >
      For each network in the 'cidr_networks' level, specify a list of IP 
      addresses
      or a range of IP addresses that the inventory generator should exclude from
      the pools of IP addresses for network interfaces inside containers. To use a
      range, specify the lower and upper IP addresses (inclusive) with a comma
      separator.
     
      Example:
     
      The management network includes a router (gateway) on 172.29.236.1 and
      DNS servers on 172.29.236.11-12. The deployment includes seven target
      servers on 172.29.236.101-103, 172.29.236.111, 172.29.236.121, and
      172.29.236.131. However, the inventory generator automatically excludes
      these IP addresses. Network policy at this particular example organization
      also reserves 231-254 in the last octet at the high end of the range for
      network device management.
     
      used_ips:
        - 172.29.236.1
        - 172.29.236.11,172.29.236.12
        - 172.29.239.231,172.29.239.254
    type: array
    # TODO(sigmavirus24): Write a format checker for this
    # items:
    #   format: used_ips

  global_overrides:
    documentation: >
      Contains global options that require customization for a deployment. For
      example, load balancer virtual IP addresses (VIP). This level also provides
      a mechanism to override other options defined in the playbook structure.
    type: object
    required:
      - internal_lb_vip_address
      - external_lb_vip_address
      - management_bridge
      - provider_networks
    properties:
      internal_lb_vip_address:
        type: string
        documentation: >
          Load balancer VIP for the following items:
       
          - Local package repository
          - Galera SQL database cluster
          - Administrative and internal API endpoints for all OpenStack services
          - Glance registry
          - Nova compute source of images
          - Cinder source of images
          - Instance metadata
      external_lb_vip_address:
        type: string
        documentation: >
          Load balancer VIP for the following items:
       
          - Public API endpoints for all OpenStack services
          - Horizon
      management_bridge:
        type: string
        documentation: >
          Name of management network bridge on target hosts. Typically 'br-mgmt'
      tunnel_bridge:
        type: string
        documentation: >
          Name of tunnel network bridge on target hosts. Typically 'br-vxlan'.
      provider_networks:
        type: array
        documentation: >
          List of container and bare metal networks on target hosts.
        items:
          type: object
          required:
            - network
          properties:
            network:
              type: object
              documentation: >
                Defines a container or bare metal network. Create a level for each
                network.
              required:
                - type
                - container_bridge
                - container_interface
                - container_type
                # - is_container_address
                # - is_ssh_address
                - group_binds
              properties:
                type:
                  type: string
                  documentation: >
                    Type of network. Networks other than those for neutron such as
                    management and storage typically use 'raw'. Neutron networks use
                    'flat', 'vlan', or 'vxlan'. Coincides with ML2 plug-in configuration
                    options.
                container_bridge:
                  type: string
                  documentation: >
                    Name of unique bridge on target hosts to use for this network. Typical
                    values include 'br-mgmt', 'br-storage', 'br-vlan', 'br-vxlan', etc.
                container_interface:
                  type: string
                  documentation: >
                    Name of unique interface in containers to use for this network.
                    Typical values include 'eth1', 'eth2', etc.
                container_type:
                  type: string
                  documentation: >
                    Name of mechanism that connects interfaces in containers to the bridge
                    on target hosts for this network. Typically 'veth'.
                container_mtu:
                  type: string
                  documentation: >
                    Sets the MTU within LXC for a given network type.
                ip_from_q:
                  type: string
                  documentation: >
                    Name of network in 'cidr_networks' level to use for IP address pool.
                    Only valid for 'raw' and 'vxlan' types.
                is_container_address:
                  type: boolean
                  documentation: >
                    If true, the load balancer uses this IP address to access services
                    in the container. Only valid for networks with 'ip_from_q' option.
                is_ssh_address:
                  type: boolean
                  documentation: >
                    If true, Ansible uses this IP address to access the container via SSH.
                    Only valid for networks with 'ip_from_q' option.
                group_binds:
                  type: array
                  documentation: >
                    List of one or more Ansible groups that contain this
                    network. For more information, see the openstack_environment.yml file.
                  items:
                    type: string
                net_name:
                  type: string
                  documentation: >
                    Name of network for 'flat' or 'vlan' types. Only valid for these
                    types. Coincides with ML2 plug-in configuration options.
                range:
                  type: string
                  documentation: >
                    For 'vxlan' type neutron networks, range of VXLAN network identifiers
                    (VNI). For 'vlan' type neutron networks, range of VLAN tags. Coincides
                    with ML2 plug-in configuration options.
    examples:
      - scenario: >
          Define a typical network architecture:
       
          - Network of type 'raw' that uses the 'br-mgmt' bridge and 'management'
            IP address pool. Maps to the 'eth1' device in containers. Applies to all
            containers and bare metal hosts. Both the load balancer and Ansible
            use this network to access containers and services.
          - Network of type 'raw' that uses the 'br-storage' bridge and 'storage'
            IP address pool. Maps to the 'eth2' device in containers. Applies to
            nova compute and all storage service containers. Optionally applies to
            to the swift proxy service.
          - Network of type 'vxlan' that contains neutron VXLAN tenant networks
            1 to 1000 and uses 'br-vxlan' bridge on target hosts. Maps to the
            'eth10' device in containers. Applies to all neutron agent containers
            and neutron agents on bare metal hosts.
          - Network of type 'vlan' that contains neutron VLAN networks 101 to 200
            and uses the 'br-vlan' bridge on target hosts. Maps to the 'eth11' device
            in containers. Applies to all neutron agent containers and neutron agents
            on bare metal hosts.
          - Network of type 'flat' that contains one neutron flat network and uses
            the 'br-vlan' bridge on target hosts. Maps to the 'eth12' device in
            containers. Applies to all neutron agent containers and neutron agents
            on bare metal hosts.
       
          Note: A pair of 'vlan' and 'flat' networks can use the same bridge because
          one only handles tagged frames and the other only handles untagged frames
          (the native VLAN in some parlance). However, additional 'vlan' or 'flat'
          networks require additional bridges.
        example: |
          provider_networks:
            - network:
                group_binds:
                  - all_containers
                  - hosts
                type: "raw"
                container_bridge: "br-mgmt"
                container_interface: "eth1"
                container_type: "veth"
                ip_from_q: "management"
                is_container_address: true
                is_ssh_address: true
            - network:
                group_binds:
                  - glance_api
                  - cinder_api
                  - cinder_volume
                  - nova_compute
                  # Uncomment the next line if using swift with a storage network.
                  # - swift_proxy
                type: "raw"
                container_bridge: "br-storage"
                container_type: "veth"
                container_interface: "eth2"
                container_mtu: "9000"
                ip_from_q: "storage"
            - network:
                group_binds:
                  - neutron_linuxbridge_agent
                container_bridge: "br-vxlan"
                container_type: "veth"
                container_interface: "eth10"
                container_mtu: "9000"
                ip_from_q: "tunnel"
                type: "vxlan"
                range: "1:1000"
                net_name: "vxlan"
            - network:
                group_binds:
                  - neutron_linuxbridge_agent
                container_bridge: "br-vlan"
                container_type: "veth"
                container_interface: "eth11"
                type: "vlan"
                range: "101:200"
                net_name: "vlan"
            - network:
                group_binds:
                  - neutron_linuxbridge_agent
                container_bridge: "br-vlan"
                container_type: "veth"
                container_interface: "eth12"
                host_bind_override: "eth12"
                type: "flat"
                net_name: "flat"
      - scenario: >
          Add the service network to the previous example:
       
          - Network of type 'raw' that uses the 'br-snet' bridge and 'snet' IP
            address pool. Maps to the 'eth3' device in containers. Applies to
            glance, nova compute, neutron agent containers, and any of these
            services on bare metal hosts.
        example: |
          provider_networks:
            - network:
                group_binds:
                  - glance_api
                  - nova_compute
                  - neutron_linuxbridge_agent
                type: "raw"
                container_bridge: "br-snet"
                container_type: "veth"
                container_interface: "eth3"
                ip_from_q: "snet"

  shared-infra_hosts:
    type: object
    documentation: >
      List of target hosts on which to deploy shared infrastructure services
      including the Galera SQL database cluster, RabbitMQ, and Memcached. Recommend
      three minimum target hosts for these services.
    $ref: '#/common_definitions/infra_hosts'
    examples:
      - scenario: >
          Define three shared infrastructure hosts:
        example: |
          shared-infra_hosts:
            infra1:
              ip: 172.29.236.101
            infra2:
              ip: 172.29.236.102
            infra3:
              ip: 172.29.236.103

  repo-infra_hosts:
    type: object
    documentation: >
      List of target hosts on which to deploy the package repository. Recommend
      minimum three target hosts for this service. Typically contains the same
      target hosts as the 'shared-infra_hosts' level.
    $ref: '#/common_definitions/infra_hosts'
    examples:
      - scenario: >
          Define three package repository hosts:
        example: |
          repo-infra_hosts:
            infra1:
              ip: 172.29.236.101
            infra2:
              ip: 172.29.236.102
            infra3:
              ip: 172.29.236.103

  os-infra_hosts:
    type: object
    documentation: >
      List of target hosts on which to deploy the glance API, nova API, heat API,
      and horizon. Recommend three minimum target hosts for these services.
      Typically contains the same target hosts as 'shared-infra_hosts' level.
    $ref: '#/common_definitions/infra_hosts'
    examples:
      - scenario: >
          Define three OpenStack infrastructure hosts:
        example: |
          os-infra_hosts:
            infra1:
              ip: 172.29.236.100
            infra2:
              ip: 172.29.236.101
            infra3:
              ip: 172.29.236.102

  identity_hosts:
    type: object
    documentation: >
      List of target hosts on which to deploy the keystone service. Recommend
      three minimum target hosts for this service. Typically contains the same
      target hosts as the 'shared-infra_hosts' level.
    $ref: '#/common_definitions/infra_hosts'
    examples:
      - scenario: >
          Define three OpenStack identity hosts:
        example: |
          identity_hosts:
            infra1:
              ip: 172.29.236.101
            infra2:
              ip: 172.29.236.102
            infra3:
              ip: 172.29.236.103

  network_hosts:
    type: object
    documentation: >
      List of target hosts on which to deploy neutron services. Recommend three
      minimum target hosts for this service. Typically contains the same target
      hosts as the 'shared-infra_hosts' level.
    $ref: '#/common_definitions/infra_hosts'
    examples:
      - scenario: >
          Define three OpenStack network hosts:
        example: |
          network_hosts:
            infra1:
              ip: 172.29.236.101
            infra2:
              ip: 172.29.236.102
            infra3:
              ip: 172.29.236.103

  compute_hosts:
    type: object
    documentation: >
      List of target hosts on which to deploy the nova compute service. Recommend
      one minimum target host for this service. Typically contains target hosts
      that do not reside in other levels.
    $ref: '#/common_definitions/infra_hosts'
    examples:
      - scenario: >
          Define three OpenStack compute hosts:
        example: |
          compute_hosts:
            infra1:
              ip: 172.29.236.101
            infra2:
              ip: 172.29.236.102
            infra3:
              ip: 172.29.236.103

  storage-infra_hosts:
    type: object
    documentation: >
      List of target hosts on which to deploy the cinder API. Recommend three
      minimum target hosts for this service. Typically contains the same target
      hosts as the 'shared-infra_hosts' level.
    $ref: '#/common_definitions/infra_hosts'
    examples:
      - scenario: >
          Define three OpenStack storage hosts:
        example: |
          storage-infra_hosts:
            infra1:
              ip: 172.29.236.101
            infra2:
              ip: 172.29.236.102
            infra3:
              ip: 172.29.236.103

  storage_hosts:
    type: object
    documentation: >
      List of target hosts on which to deploy the cinder volume service. Recommend
      one minimum target host for this service. Typically contains target hosts
      that do not reside in other levels.
    # TODO(sigmavirus24): Figure out how to make a patternProperty requried
    patternProperties:
      '[a-zA-Z0-9]+':
        type: object
        documentation: >
          Hostname of a target host.
        $ref: '#/common_definitions/storage_host'
    examples:
      - scenario: >
          Define an OpenStack storage host:
        example: |
          storage_hosts:
            storage1:
              ip: 172.29.236.121
      - scenario: >
          Use the LVM iSCSI backend in availability zone 'cinderAZ_1':
        example: |
          container_vars:
            cinder_storage_availability_zone: cinderAZ_1
            cinder_default_availability_zone: cinderAZ_1
            cinder_backends:
              lvm:
                volume_backend_name: LVM_iSCSI
                volume_driver: cinder.volume.drivers.lvm.LVMVolumeDriver
                volume_group: cinder-volumes
              limit_container_types: cinder_volume
      - scenario: >
          Use the NetApp iSCSI backend via Data ONTAP 7-mode in availability zone
          'cinderAZ_2':
        example: |
          container_vars:
            cinder_storage_availability_zone: cinderAZ_2
            cinder_default_availability_zone: cinderAZ_1
            cinder_backends:
              netapp:
                volume_backend_name: NETAPP_iSCSI
                volume_driver: cinder.volume.drivers.netapp.common.NetAppDriver
                netapp_storage_family: ontap_7mode
                netapp_storage_protocol: iscsi
                netapp_server_hostname: hostname
                netapp_server_port: 443
                netapp_login: username
                netapp_password: password

  log_hosts:
    type: object
    documentation: >
      List of target hosts on which to deploy logging services. Recommend
      one minimum target host for this service.
    $ref: '#/common_definitions/infra_hosts'
    examples:
      - scenario: >
          Define a logging host:
        example: |
          log_hosts:
            log1:
              ip: 172.29.236.131
